# mlp

> 20250423 14:43

做了256组实验，效果出奇的好，比之前的 bow 和 bigram 都要好上很多

- model x 1: mlp
- n_embd x 4: 32,64,128,256
- n_embd2 x 4: 32,64,128,256
- learning_rate x 4: 1e-2,1e-3,3e-4,5e-4
- block_size x 4: 8,16,32,64

## 结论1 n_embd

- 比较玄学，直觉想法是这个参数变大应该是会变好的，但也需要学习率的配合才行

| Experiment   |   *n_embd* |   n_embd2 |   block_size |   learning_rate |   Final/train_loss |   Final/val_loss |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |       64 |       256 |           32 |          0.001  |            1.52627 |          1.78516 |
| mlp_exp      |      128 |       256 |           32 |          0.001  |            1.54497 |          1.80563 |
| mlp_exp      |       32 |       256 |           32 |          0.001  |            1.57205 |          1.80633 |
| mlp_exp      |      256 |       256 |           32 |          0.001  |            1.59237 |          1.82917 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      128 |       256 |           32 |          0.0005 |            1.53872 |          1.78581 |
| mlp_exp      |       64 |       256 |           32 |          0.0005 |            1.55412 |          1.78773 |
| mlp_exp      |      256 |       256 |           32 |          0.0005 |            1.55624 |          1.7999  |
| mlp_exp      |       32 |       256 |           32 |          0.0005 |            1.62719 |          1.82584 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      256 |       256 |           32 |          0.0003 |            1.55681 |          1.79116 |
| mlp_exp      |      128 |       256 |           32 |          0.0003 |            1.5661  |          1.79432 |
| mlp_exp      |       64 |       256 |           32 |          0.0003 |            1.60407 |          1.8083  |
| mlp_exp      |       32 |       256 |           32 |          0.0003 |            1.69799 |          1.86872 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      128 |       256 |           16 |          0.001  |            1.63154 |          1.81924 |
| mlp_exp      |       64 |       256 |           16 |          0.001  |            1.63749 |          1.82486 |
| mlp_exp      |      256 |       256 |           16 |          0.001  |            1.65791 |          1.84475 |
| mlp_exp      |       32 |       256 |           16 |          0.001  |            1.6741  |          1.85388 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      128 |       256 |           64 |          0.0003 |            1.45989 |          1.8204  |
| mlp_exp      |       64 |       256 |           64 |          0.0003 |            1.48863 |          1.82244 |
| mlp_exp      |      256 |       256 |           64 |          0.0003 |            1.47397 |          1.83751 |
| mlp_exp      |       32 |       256 |           64 |          0.0003 |            1.58248 |          1.84051 |

## 结论2 n_embd2 越大越好

在固定 n_embd 的情况下，我们发现 n_embd2 越大越好，这和我们的直观想法是一致的，这一层相当于 hidden layer 的大小，越大越能拟合复杂的函数。

| Experiment   |   n_embd |   block_size |   learning_rate |   *n_embd2* |   Final/train_loss |   Final/val_loss |
|--------------|----------|--------------|-----------------|-----------|--------------------|------------------|
| mlp_exp      |       64 |           32 |          0.001  |       256 |            1.52627 |          1.78516 |
| mlp_exp      |       64 |           32 |          0.001  |       128 |            1.62799 |          1.84789 |
| mlp_exp      |       64 |           32 |          0.001  |        64 |            1.73213 |          1.91425 |
| mlp_exp      |       64 |           32 |          0.001  |        32 |            1.88341 |          2.03176 |
|--------------|----------|--------------|-----------------|-----------|--------------------|------------------|
| mlp_exp      |      128 |           32 |          0.0005 |       256 |            1.53872 |          1.78581 |
| mlp_exp      |      128 |           32 |          0.0005 |       128 |            1.63364 |          1.8226  |
| mlp_exp      |      128 |           32 |          0.0005 |        64 |            1.75231 |          1.91336 |
| mlp_exp      |      128 |           32 |          0.0005 |        32 |            1.90318 |          2.02658 |
|--------------|----------|--------------|-----------------|-----------|--------------------|------------------|
| mlp_exp      |       64 |           32 |          0.0005 |       256 |            1.55412 |          1.78773 |
| mlp_exp      |       64 |           32 |          0.0005 |       128 |            1.66111 |          1.85033 |
| mlp_exp      |       64 |           32 |          0.0005 |        64 |            1.7727  |          1.93314 |
| mlp_exp      |       64 |           32 |          0.0005 |        32 |            1.91734 |          2.04505 |
|--------------|----------|--------------|-----------------|-----------|--------------------|------------------|
| mlp_exp      |      256 |           32 |          0.0003 |       256 |            1.55681 |          1.79116 |
| mlp_exp      |      256 |           32 |          0.0003 |       128 |            1.65692 |          1.84989 |
| mlp_exp      |      256 |           32 |          0.0003 |        64 |            1.77382 |          1.92353 |
| mlp_exp      |      256 |           32 |          0.0003 |        32 |            1.937   |          2.05376 |

## 结论3 block_size 不一定越大越好

- 基本是32的情况最好

| Experiment   |   n_embd |   n_embd2 |   learning_rate |   *block_size* |   Final/train_loss |   Final/val_loss |
|--------------|----------|-----------|-----------------|--------------|--------------------|------------------|
| mlp_exp      |       64 |       256 |          0.001  |           32 |            1.52627 |          1.78516 |
| mlp_exp      |       64 |       256 |          0.001  |           16 |            1.63749 |          1.82486 |
| mlp_exp      |       64 |       256 |          0.001  |           64 |            1.42078 |          1.86692 |
| mlp_exp      |       64 |       256 |          0.001  |            8 |            1.78932 |          1.92955 |
|--------------|----------|-----------|-----------------|--------------|--------------------|------------------|
| mlp_exp      |      128 |       256 |          0.0005 |           32 |            1.53872 |          1.78581 |
| mlp_exp      |      128 |       256 |          0.0005 |           16 |            1.64647 |          1.82186 |
| mlp_exp      |      128 |       256 |          0.0005 |           64 |            1.44139 |          1.84164 |
| mlp_exp      |      128 |       256 |          0.0005 |            8 |            1.79824 |          1.93394 |
|--------------|----------|-----------|-----------------|--------------|--------------------|------------------|
| mlp_exp      |       64 |       256 |          0.0005 |           32 |            1.55412 |          1.78773 |
| mlp_exp      |       64 |       256 |          0.0005 |           64 |            1.44122 |          1.83248 |
| mlp_exp      |       64 |       256 |          0.0005 |           16 |            1.6803  |          1.85131 |
| mlp_exp      |       64 |       256 |          0.0005 |            8 |            1.83225 |          1.95801 |
|--------------|----------|-----------|-----------------|--------------|--------------------|------------------|
| mlp_exp      |      256 |       256 |          0.0003 |           32 |            1.55681 |          1.79116 |
| mlp_exp      |      256 |       256 |          0.0003 |           16 |            1.66052 |          1.82967 |
| mlp_exp      |      256 |       256 |          0.0003 |           64 |            1.47397 |          1.83751 |
| mlp_exp      |      256 |       256 |          0.0003 |            8 |            1.80259 |          1.93064 |

## 结论4 learning_rate

- 玄学参数，没有看出什么规律

| Experiment   |   n_embd |   n_embd2 |   block_size |   *learning_rate* |   Final/train_loss |   Final/val_loss |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |       64 |       256 |           32 |          0.001  |            1.52627 |          1.78516 |
| mlp_exp      |       64 |       256 |           32 |          0.0005 |            1.55412 |          1.78773 |
| mlp_exp      |       64 |       256 |           32 |          0.0003 |            1.60407 |          1.8083  |
| mlp_exp      |       64 |       256 |           32 |          0.01   |            1.68018 |          1.90789 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      128 |       256 |           32 |          0.0005 |            1.53872 |          1.78581 |
| mlp_exp      |      128 |       256 |           32 |          0.0003 |            1.5661  |          1.79432 |
| mlp_exp      |      128 |       256 |           32 |          0.001  |            1.54497 |          1.80563 |
| mlp_exp      |      128 |       256 |           32 |          0.01   |            1.79259 |          1.99425 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |      256 |       256 |           32 |          0.0003 |            1.55681 |          1.79116 |
| mlp_exp      |      256 |       256 |           32 |          0.0005 |            1.55624 |          1.7999  |
| mlp_exp      |      256 |       256 |           32 |          0.001  |            1.59237 |          1.82917 |
| mlp_exp      |      256 |       256 |           32 |          0.01   |            1.91922 |          2.08698 |
|--------------|----------|-----------|--------------|-----------------|--------------------|------------------|
| mlp_exp      |       32 |       256 |           32 |          0.001  |            1.57205 |          1.80633 |
| mlp_exp      |       32 |       256 |           32 |          0.0005 |            1.62719 |          1.82584 |
| mlp_exp      |       32 |       256 |           32 |          0.0003 |            1.69799 |          1.86872 |
| mlp_exp      |       32 |       256 |           32 |          0.01   |            1.63259 |          1.87969 |

## Appendix

### 训练脚本

```bash
python train.py --multirun \
    hydra=multi_run \
    model=mlp \
    model.n_embd=32,64,128,256 \
    model.n_embd2=32,64,128,256 \
    logger.name=mlp_exp \
    training.learning_rate=1e-2,1e-3,3e-4,5e-4 \
    training.block_size=8,16,32,64 \
```

### 模型代码

```python
import torch
import torch.nn as nn
from torch.nn import functional as F

__all__ = ['MLP']

# -----------------------------------------------------------------------------
# MLP language model

class MLP(nn.Module):
    """
    takes the previous block_size tokens, encodes them with a lookup table,
    concatenates the vectors and predicts the next token with an MLP.

    Reference:
    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
    """

    def __init__(self, n_embd:int, n_embd2:int, block_size:int, vocab_size:int):
        super().__init__()
        self.block_size = block_size
        self.vocab_size = vocab_size
        self.wte = nn.Embedding(vocab_size + 1, n_embd) # token embeddings table
        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token
        # before the beginning of the input sequence
        self.mlp = nn.Sequential(
            nn.Linear(self.block_size * n_embd, n_embd2),
            nn.Tanh(),
            nn.Linear(n_embd2, self.vocab_size)
        )

    def get_block_size(self):
        return self.block_size

    def forward(self, idx, targets=None):

        # gather the word embeddings of the previous 3 words
        embs = []
        for k in range(self.block_size):
            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)
            idx = torch.roll(idx, 1, 1)
            idx[:, 0] = self.vocab_size # special <BLANK> token
            embs.append(tok_emb)

        # concat all of the embeddings together and pass through an MLP
        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)
        logits = self.mlp(x)

        # if we are given some desired targets also calculate the loss
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss
```
